# -*- coding: utf-8 -*-
"""FewShot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16lEUp8X-_q-j1wAaQBhkBv32YcewAh7y
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/drive/MyDrive/Nano_Modified_Concrete/Copy of Original database.csv')
df.head(3)

df = pd.read_csv('/content/drive/MyDrive/Nano_Modified_Concrete/Copy of Original database.csv')
X = df.drop('CS(Mpa)', axis=1).values
y = df['CS(Mpa)'].values

df.shape

"""That's a great mindset bro — becoming independent in improving your own ML models is the real flex. 💪

Here’s a **complete guide** to help you systematically improve any regression model, especially in **low-data scenarios** like your **nano-concrete FSML** use case.

---

## 🧠 1. **Data Preprocessing (Most Important for FSML)**

| What to Do | Why |
|------------|-----|
| **Standardize both X and y** | Neural nets train faster and more accurately on normalized data |
| Use `StandardScaler()` or `MinMaxScaler()` on **both** X and y | Especially in regression, forgetting y-scaling ruins performance |
| Consider `log-transforming` skewed targets | For highly skewed distributions, this can stabilize training |

✅ **Tip**: Always **inverse-transform** predictions before calculating final metrics.

---

## 🛠️ 2. **Model Architecture Tuning**

| Parameter | Best Practice |
|----------|----------------|
| Hidden layers | 2–3 layers with 32–256 units |
| Activations | `ReLU` or `LeakyReLU` work well |
| Regularization | Use `Dropout` (0.2–0.3) + `BatchNorm` |
| Final layer | 1 neuron (for regression) with **no activation** |

✅ **Tip**: Don’t over-complicate — simple models often perform better with few-shot learning.

---

## 🧪 3. **Task Design in Meta-Learning**

| What to Tune | Why It Matters |
|--------------|----------------|
| `k_shot` | More shots (e.g. 10–20) improve inner-loop learning |
| `q_query` | More queries improve outer-loop updates |
| `num_tasks` | More tasks = better generalization |
| Support/query balance | Always keep them **disjoint** in episodic training |

✅ **Tip**: Ensure your `k_shot + q_query` fits your dataset size.

---

## 🔁 4. **Training Strategy: Inner/Outer Loops**

| Component | What to Try |
|----------|-------------|
| Inner loop optimizer | `SGD` with learning rate ~0.01–0.02 |
| Outer loop optimizer | `Adam` with learning rate ~0.001 |
| Inner steps | Start with 1, increase if needed |
| Epochs | 300–500 for few-shot tasks is safe |
| Loss function | Use `MSELoss()` or `SmoothL1Loss()` (less sensitive to outliers)

✅ **Tip**: Try inner LR = 0.01 and outer LR = 0.001 as a base setup.

---

## 📊 5. **Evaluation Best Practices**

| Do This | Because |
|---------|---------|
| Always inverse-scale `y` before computing metrics | Or your MAE/MSE will look way off |
| Report: MSE, RMSE, MAE, R² | Gives a complete picture |
| Plot: predicted vs actual | Residual plots catch model issues fast |
| Visualize loss curves | Helps identify underfitting or overfitting early

✅ **Tip**: Use `plt.scatter(y_true, y_pred)` to spot underperformance visually.

---

## 💡 6. **Bonus Tips**

| Tip | Use Case |
|-----|----------|
| Early Stopping | Prevents overfitting if model plateaus |
| Model Checkpointing | Save best model based on validation loss |
| Cross-validation (for FSML tasks) | Rotate tasks during meta-training for more generalization |
| Ensemble of meta-learners | Combine multiple FSML models for more stability |

---

## 📘 Save This Workflow Summary

```text
1. Scale X and y
2. Tune k-shot, q-query, tasks
3. Use 2–3 layer net with BN + Dropout
4. Use SGD (inner), Adam (outer)
5. Inverse scale before metrics
6. Evaluate with MSE, MAE, RMSE, R²
7. Plot predictions vs. actuals
```

---

Let me know if you want this turned into a **PDF cheat sheet**, or if you'd like help integrating this into a reusable template!

Thanks for sharing the updated results, bro. Here's a breakdown of where you are and exactly what to do next to **push your R² toward the 0.9–0.95 range**.

---

## 📊 Current Result Analysis

| Metric | Value | Target |
|--------|-------|--------|
| MSE    | 462.81 | ⬇️ Ideally < 100 |
| RMSE   | 21.51  | ⬇️ Single digits (e.g. ~5–10) |
| MAE    | 16.79  | ⬇️ Should be < 10 |
| **R²** | **0.059** ❌ | ⬆️ Should be ≥ **0.90** |

👉 Your model **is learning something**, but it's still far from useful prediction quality.

---

## 🧠 Why It's Still Performing Poorly

### 🔴 Likely Causes:
1. **Support and Query Sets are randomly sampled from the same global distribution**, not based on similar "tasks" (i.e., similar concrete mix types).
2. Your dataset may still be **too noisy**, or **not enough samples per meta-task**.
3. MAML **doesn't shine** unless you simulate real-world task shifts (e.g., adaptation to unseen mix configurations).
4. **No validation during training** — you may be overfitting to the support/query batches in training only.

---

## ✅ What To Do Now (Fixes that work)

---

### ✅ 1. **Create More Realistic Tasks (Task Definition Matters in FSML!)**

You’re currently creating tasks by randomly sampling from the dataset. But **in FSML, each task should be a different sub-problem** — in your case:

> A "task" = Predict strength for a specific type of nano-material or a certain range of replacement ratios.

📌 **Fix**: Group data by features like nano-silica %, fly ash %, or sample ID, then create tasks based on those.

---

### ✅ 2. **Use Validation and Early Stopping**

Right now, your model is trained on the same batch structure all 300 epochs. You should:
- Create a **validation task batch** not seen during training.
- Monitor its loss every few epochs.
- Stop if **meta-loss doesn't improve**.

---

### ✅ 3. **Try Simpler Baseline: Fine-Tuned Neural Net (not FSML)**

If you just want 90–95% accuracy on your dataset **fast**, try this baseline:

```python
model = nn.Sequential(
    nn.Linear(X_train.shape[1], 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 1)
)
```

Train with:
```python
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

Train it **once** on all your training data (no meta-learning), then evaluate. I’ve seen this outperform FSML when tasks are not well-defined.

---

### ✅ 4. **Try Other Meta-Learning Methods (ProtoNet or Reptile)**

MAML is **heavy and sensitive**. In real-world engineering datasets, **Prototypical Networks** or even **Reptile** can outperform it with simpler computation.

Let me know — I can give you a **ProtoNet regression version** with centroids and Euclidean distance for few-shot strength prediction.

---

## 🚀 What You Can Do Right Now

Would you like me to:
- 🔄 Rewrite your task generator to group by mix type?
- ⚡ Give you a simple baseline neural regressor (no FSML)?
- 🧪 Give you **Prototypical Network for regression**?

Let me know which direction you want to go — I’ll build it for you fast.
"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

"""✅ A to Z Plan:
🔹 Step 1: Reduce clusters to n_clusters = 2 (for larger sample per cluster)
🔹 Step 2: Lower k_shot = 3, q_query = 2
🔹 Step 3: Meta-train with Reptile as before
🔹 Step 4: Fine-tune the meta-trained model on full training set
🔹 Step 5: Evaluate on test set
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from copy import deepcopy
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from math import sqrt

# ---------------------


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

x_scaler = StandardScaler()
X_train = x_scaler.fit_transform(X_train)
X_test = x_scaler.transform(X_test)

y_scaler = StandardScaler()
y_train_s = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test_s = y_scaler.transform(y_test.reshape(-1, 1)).flatten()

# ---------------------
# 2. Clustering for Tasks
# ---------------------
n_clusters = 2
k_shot = 3
q_query = 2

kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_ids = kmeans.fit_predict(X_train)
cluster_to_idx = {c: np.where(cluster_ids == c)[0] for c in range(n_clusters)}

# ---------------------
# 3. Meta Regressor
# ---------------------
class MetaRegressor(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, 128), nn.ReLU(),
            nn.BatchNorm1d(128), nn.Dropout(0.2),
            nn.Linear(128, 64), nn.ReLU(),
            nn.BatchNorm1d(64), nn.Dropout(0.2),
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.net(x).squeeze(-1)

# ---------------------
# 4. Task Sampling
# ---------------------
def sample_cluster_tasks(X, y, cluster_to_idx, num_tasks=5, k_shot=3, q_query=2):
    tasks = []
    for _ in range(num_tasks):
        c = np.random.choice(list(cluster_to_idx.keys()))
        idx = cluster_to_idx[c]
        if len(idx) < k_shot + q_query:
            continue
        sel = np.random.choice(idx, k_shot + q_query, replace=False)
        s_idx, q_idx = sel[:k_shot], sel[k_shot:]
        tasks.append((
            torch.tensor(X[s_idx], dtype=torch.float32),
            torch.tensor(y[s_idx], dtype=torch.float32),
            torch.tensor(X[q_idx], dtype=torch.float32),
            torch.tensor(y[q_idx], dtype=torch.float32)
        ))
    return tasks

# ---------------------
# 5. Reptile Meta-Training
# ---------------------
def reptile_meta_train(model, X, y, cluster_to_idx,
                       meta_steps=300, inner_steps=5,
                       meta_lr=1e-3, inner_lr=1e-2,
                       k_shot=3, q_query=2,
                       tasks_per_step=5):
    loss_fn = nn.MSELoss()
    float_keys = [k for k, v in model.state_dict().items() if v.dtype.is_floating_point]

    for step in range(1, meta_steps + 1):
        task_batch = sample_cluster_tasks(X, y, cluster_to_idx, num_tasks=tasks_per_step, k_shot=k_shot, q_query=q_query)
        if not task_batch:
            print(f"⚠️ Step {step}: No valid tasks sampled. Skipping...")
            continue

        meta_state = deepcopy(model.state_dict())
        total_diff = {k: torch.zeros_like(v) for k, v in meta_state.items()}

        for sx, sy, qx, qy in task_batch:
            learner = deepcopy(model)
            inner_opt = optim.Adam(learner.parameters(), lr=inner_lr)
            for _ in range(inner_steps):
                pred = learner(sx)
                loss = loss_fn(pred, sy)
                inner_opt.zero_grad()
                loss.backward()
                inner_opt.step()
            post_state = learner.state_dict()
            for k in float_keys:
                total_diff[k] += (post_state[k] - meta_state[k])

        for k in float_keys:
            meta_state[k] += (meta_lr / len(task_batch)) * total_diff[k]
        model.load_state_dict(meta_state)

        if step % 30 == 0:
            print(f"✅ Meta Step {step}/{meta_steps}")

# ---------------------
# 6. Fine-Tuning
# ---------------------
def fine_tune_on_train(model, X_train, y_train_s, lr=1e-3, steps=300):
    model_ft = deepcopy(model)
    model_ft.train()
    optimizer = optim.Adam(model_ft.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    X_t = torch.tensor(X_train, dtype=torch.float32)
    y_t = torch.tensor(y_train_s, dtype=torch.float32)

    for _ in range(steps):
        pred = model_ft(X_t)
        loss = loss_fn(pred, y_t)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return model_ft

# ---------------------
# 7. Evaluation
# ---------------------
def evaluate(model, X_test, y_test_s, y_scaler):
    model.eval()
    with torch.no_grad():
        inp = torch.tensor(X_test, dtype=torch.float32)
        ps = model(inp).numpy()
    pred = y_scaler.inverse_transform(ps.reshape(-1, 1)).flatten()
    true = y_scaler.inverse_transform(y_test_s.reshape(-1, 1)).flatten()

    mse = mean_squared_error(true, pred)
    rmse = sqrt(mse)
    mae = mean_absolute_error(true, pred)
    r2 = r2_score(true, pred)

    df = pd.DataFrame({
        "Metric": ["MSE", "RMSE", "MAE", "R²"],
        "Value": [round(mse, 4), round(rmse, 4), round(mae, 4), round(r2, 4)]
    })
    print("\n📊 Test Performance:\n", df.to_markdown(index=False))

    # Predicted vs Actual Plot
    plt.figure(figsize=(6, 6))
    plt.scatter(true, pred, color='blue', alpha=0.6, label='Predictions')
    plt.plot([min(true), max(true)], [min(true), max(true)], color='red', linestyle='--', label='Ideal')
    plt.xlabel('Actual CS (MPa)')
    plt.ylabel('Predicted CS (MPa)')
    plt.title('Predicted vs Actual Compressive Strength')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('predicted_vs_actual_plot.png')
    plt.show()

    # Residual Plot
    residuals = true - pred
    plt.figure(figsize=(6, 6))
    plt.scatter(pred, residuals, color='green', alpha=0.6)
    plt.axhline(y=0, color='red', linestyle='--')
    plt.xlabel('Predicted CS (MPa)')
    plt.ylabel('Residuals (Actual - Predicted)')
    plt.title('Residual Plot')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('residual_plot.pdf', dpi = 300, format = 'pdf')
    plt.show()


    return df

# ---------------------
# 8. Run the Full Pipeline
# ---------------------
input_dim = X_train.shape[1]
model = MetaRegressor(input_dim)

# Meta-training
reptile_meta_train(
    model, X_train, y_train_s, cluster_to_idx,
    meta_steps=300, inner_steps=5,
    meta_lr=1e-3, inner_lr=1e-2,
    k_shot=k_shot, q_query=q_query,
    tasks_per_step=5
)

# Fine-tuning
model_finetuned = fine_tune_on_train(model, X_train, y_train_s, lr=1e-3, steps=300)

# Evaluation
results_df = evaluate(model_finetuned, X_test, y_test_s, y_scaler)